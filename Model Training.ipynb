{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3c82a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\benan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras.applications import MobileNetV2  # 1. Import the MobileNetV2 model\n",
    "from tensorflow.keras.layers import AveragePooling2D # Import the AveragePooling2D layer\n",
    "from tensorflow.keras.layers import Dropout # Import the Dropout layer\n",
    "from tensorflow.keras.layers import Flatten # Import the Flatten layer\n",
    "from tensorflow.keras.layers import Dense # Import the Dense layer\n",
    "from tensorflow.keras.layers import Input # Import the Input layer\n",
    "from tensorflow.keras.models import Model # Import the Model layer\n",
    "from tensorflow.keras.optimizers.legacy import Adam  # 2. Import the Adam optimizer\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input  # 3. Import the preprocess_input function from the mobilenet_v2 module\n",
    "from tensorflow.keras.preprocessing.image import img_to_array  # 4. Import the img_to_array function from the keras.preprocessing.image module\n",
    "from tensorflow.keras.preprocessing.image import load_img  # 5. Import the load_img function from the keras.preprocessing.image module\n",
    "from tensorflow.keras.utils import to_categorical  # 6. Import the to_categorical function from the keras.utils module\n",
    "from sklearn.preprocessing import LabelBinarizer  # 7. Import the LabelBinarizer class from the sklearn.preprocessing module\n",
    "from sklearn.model_selection import train_test_split  # 8. Import the train_test_split function from the sklearn.model_selection module\n",
    "from sklearn.metrics import classification_report  # 9. Import the classification_report function from the sklearn.metrics module\n",
    "from imutils import paths  # 10. Import the paths module from the imutils package\n",
    "import matplotlib.pyplot as plt  # 11. Import the pyplot module from the matplotlib package\n",
    "import numpy as np  # 12. Import the numpy module \n",
    "import argparse  # 13. Import the argparse module\n",
    "import os  # 14. Import the os module\n",
    "import scipy  # 15. Import the scipy module\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9318ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = {\n",
    "    \"dataset\": \"Dataset\",\n",
    "    \"plot\": \"plot.png\",\n",
    "    \"model\": \"mask_detector.model\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51dac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the initial learning rate, number of epochs to train for,\n",
    "# and batch size\n",
    "INIT_LR = 1e-4  # 1. Set the initial learning rate to 0.0001\n",
    "EPOCHS = 20  # 2. Set the number of epochs to 20\n",
    "BS = 32  # 3. Set the batch size to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809fc33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "# grab the list of images in our dataset directory, then initialize\n",
    "# the list of data (i.e., images) and class images\n",
    "print(\"[INFO] loading images...\")  # 4. Print a message to indicate that images are being loaded\n",
    "imagePaths = list(paths.list_images(args[\"dataset\"]))  # 5. Get a list of all the image paths in the dataset directory\n",
    "data = []  # 6. Create an empty list to store the images\n",
    "labels = []  # 7. Create an empty list to store the labels\n",
    "# loop over the image paths \n",
    "for imagePath in imagePaths:  # 8. Loop over the image paths\n",
    "    # extract the class label from the filename\n",
    "    label = imagePath.split(os.path.sep)[-2]  # 9. Extract the class label from the image path using the split method and the os.path.sep character as the separator\n",
    "    # load the input image (224x224) and preprocess it\n",
    "    image = load_img(imagePath, target_size=(224, 224))  # 10. Load the image using the load_img function from the keras.preprocessing.image module and specify the target size as (224, 224) pixels\n",
    "    image = img_to_array(image)  # 11. Convert the image to a numpy array using the img_to_array function from the keras.preprocessing.image module\n",
    "    image = preprocess_input(image)  # 12. Preprocess the image using the preprocess_input function from the mobilenet_v2 module\n",
    "    # update the data and labels lists, respectively\n",
    "    data.append(image)  # 13. Append the image to the data list\n",
    "    labels.append(label)  # 14. Append the label to the labels list\n",
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")  # 15. Convert the data list to a NumPy array using the np.array function and specify the data type as float32\n",
    "labels = np.array(labels)  # 16. Convert the labels list to a NumPy array using the np.array function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936fc13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding on the labels\n",
    "lb = LabelBinarizer()  # 17. Create a LabelBinarizer object using the LabelBinarizer class from the sklearn.preprocessing module\n",
    "labels = lb.fit_transform(labels)  # 18. Fit the LabelBinarizer object to the labels array and transform the labels array using the fit_transform method\n",
    "labels = to_categorical(labels)  # 19. Convert the labels array to a one-hot encoded NumPy array using the to_categorical function from the keras.utils module\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, \n",
    "    test_size=0.20, stratify=labels, random_state=42)  # 20. Split the data into training and testing sets using the train_test_split function from the sklearn.model_selection module and specify the test size as 20% and the random state as 42\n",
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")  # 21. Create an ImageDataGenerator object using the ImageDataGenerator class from the keras.preprocessing module and specify the rotation range, zoom range, width shift range, height shift range, shear range, horizontal flip, and fill mode as nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaf90699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\benan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "WARNING:tensorflow:From C:\\Users\\benan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the MobileNetV2 network, ensuring the head FC layer sets are\n",
    "# left off\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "    input_tensor=Input(shape=(224, 224, 3)))  # 22. Load the MobileNetV2 network using the MobileNetV2 class from the keras.applications module and specify the weights as imagenet and include the top FC layers as false and the input tensor as the shape (224, 224, 3) pixels\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output  # 23. Get the output of the base model\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)  # 24. Average pool the output of the base model using the AveragePooling2D class from the keras.layers module and specify the pool size as (7, 7)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)  # 25. Flatten the output of the AveragePooling2D layer using the Flatten class from the keras.layers module\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)  # 26. Add a dense layer with 128 neurons and the ReLU activation function\n",
    "headModel = Dropout(0.5)(headModel)  # 27. Add a dropout layer with a rate of 0.5\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)  # 28. Add a dense layer with 2 neurons and the softmax activation function\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)  # 29. Create a Model object using the Model class from the keras.models module and specify the inputs as the base model input and the outputs as the head model output\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False  # 30. Loop over all layers in the base model and freeze them so they will *not* be updated during the first training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e25418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile our model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)  # 31. Create an Adam optimizer object using the Adam class from the keras.optimizers module and specify the learning rate as INIT_LR and the decay as INIT_LR / EPOCHS\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])  # 32. Compile the model using the binary_crossentropy loss function, the Adam optimizer, and the accuracy metric\n",
    "# train the head of the network\n",
    "print(\"[INFO] training head...\")\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    validation_data=(testX, testY),\n",
    "    validation_steps=len(testX) // BS,\n",
    "    epochs=EPOCHS)  # 33. Train the head of the network using the fit method from the keras.models module and specify the training data, validation data, batch size, and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa00c06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\benan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ab09c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d33b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
